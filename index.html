<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <title>Joshua-Ren's Homepage</title>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Yi (Joshua) Ren is currently a 2nd year Ph.D. studying Machine Learning at UBC">
  <meta name="keywords" content="Yi Ren, 任毅, yiren, joshuaren, Ren, Machine Learning, Deep Learning, UBC, Computer, Vision">
  <meta name="author" content="Yi Ren" />

  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icon.png">
  <!--
  <script src="jquery.min.js"></script>
  <script>
  $(document).ready(function(){
    // Add smooth scrolling to all links
    $("a").on('click', function(event) {

      // Make sure this.hash has a value before overriding default behavior
      if (this.hash !== "") {
        // Prevent default anchor click behavior
        event.preventDefault();

        // Store hash
        var hash = this.hash;

        // Using jQuery's animate() method to add smooth page scroll
        // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
        $('html, body').animate({
          scrollTop: $(hash).offset().top
        }, 800, function(){

          // Add hash (#) to URL when done scrolling (default click behavior)
          window.location.hash = hash;
        });
      } // End if
    });
  });
  </script>
  //-->

</head>


<body class="w3-content" style="max-width:1000px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>YI REN</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button" target="_self">Home</a>
    <a href="#news" class="w3-bar-item w3-button" target="_self">News</a>
    <a href="#topics" class="w3-bar-item w3-button" target="_self">Topics</a>
    <a href="#talks" class="w3-bar-item w3-button" target="_self">Talks</a>
    <a href="#publications" class="w3-bar-item w3-button" target="_self">Publications</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">YI REN</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<head>
	<base target="_blank">
</head>
<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 60%;max-width: 240px" alt="profile photo" src="images/yiren_photo2.jpg"
	   onmouseover="this.src='images/yiren_touxiang_real2.jpg'"
	   onmouseout="this.src='images/yiren_photo2.jpg'" alt=""
	   >
      <h1>Yi (Joshua) Ren</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
			Hey, I am a Ph.D. (2020-now) student who is working on machine learning (especially learning dynamics and simplicity bias) under the supervision of 
			<a href="https://djsutherland.ml/">Prof. Danica J. Sutherland</a> at the University of British Columbia (UBC).
		        I also visited
			<a href="https://scholar.google.com/citations?user=km6CP8cAAAAJ&hl=en"> Prof. Aaron Courville's</a> group at 
			<a href="https://mila.quebec/en/">Mila</a>, working on applying iterated learning in general representation learning problems.
			Before that, I was a master's student at the University of Edinburgh, working with 	
		        <a href="https://scholar.google.com/citations?user=5_9v0CIAAAAJ">Prof. Simon Kirby</a> and
	                <a href="https://homepages.inf.ed.ac.uk/scohen/">Prof. Shay Cohen</a> on iterated learning and compositional generalization.
			I also interned at <a href="https://rbcborealis.com/"> Borealis AI </a>, working on the learning dynamics of time-series data.
        </p>
	<!--
	<p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
		<font color="#DC143C"> [On Job Market] </font> I'm looking for a research position (RA or postdoc) in the direction of 
		"<font color="#1338BE">learning dynamics, simplicity bias, </font>
		 <font color="#0492C2">compositional (systematic) generalization, </font>
		 <font color="#48AAAD">self-play-improvement in LLM, etc."</font>. Please DM or email me if you feel interested.
	</p>
	-->
        <p class="w3-center">
          <a href="mailto:renyi.joshua@gmail.com">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=5QNce38AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/Joshua-Ren">GitHub</a> &nbsp/&nbsp
	  <a href="https://twitter.com/JoshuaRenyi">Twitter</a> &nbsp/&nbsp
          <a href="data/CV_new.pdf"> CV </a>
		
        </p>
	<p class="w3-center">
	 <a href="https://ml.ubc.ca/">UBC Machine Learning</a> &nbsp/&nbsp
	 <a href="https://mild.ubc.ca/">MILD</a> &nbsp/&nbsp
	 <a href="https://amltn.cs.ubc.ca/">AML-TN</a> &nbsp/&nbsp
	</p>	
        </tbody></table>
  </div>


<!-- The Topics Section -->
  <div class="w3-container w3-light-grey w3-padding-32" id="topics">
    <h2>Intro. of My Research</h2>
    <p class="w3-justify">
        I am exploring how to train models that generalize well systematically and why effective models naturally adhere to Occam's Razor.
	This idea stems from a 
	<a href="https://cocosci.princeton.edu/tom/papers/IteratedLearningEvolutionLanguage.pdf"> talk </a>
	by Professor Simon Kirby, which discussed how the pressures of compressibility and expressivity drive human language to evolve in a more compositional direction.
	We observe that not only humans but also neural networks tend to favor highly compositional mappings when trained on various tasks.
	Such a <b>simplicity bias</b> may be progressively amplified if an intelligent agent continuously learns from the data and experiences of its predecessors,
	which is a key concept in <a href="https://cocosci.princeton.edu/tom/papers/iteratedcogsci.pdf"> Bayesian-iterated learning </a> (Bayesian-IL) in cognitive science. 
	Investigating this intriguing framework has inspired two lines of my previous research. <br><br>

	First, I worked on extending the iterated learning framework to more general deep learning systems, starting with an emergent communication setting 
	(a two-agent cooperative RL game, in Ren et al., ICLR 2020) and progressing to broader representation learning problems, including vision, language, 
	and even molecular graphs (in Ren et al., NeurIPS 2023). 
	The latter was achieved during my visit to Professor Aaron’s group at Mila. 
	We discovered that a bottleneck in the network structure plays a critical role in introducing implicit bias, which is further amplified through multi-generation self-play.
	Additionally, our recent work demonstrates that Bayesian-IL partially explains the evolution of large language models (LLMs) in pervasive self-play (in Ren et al., NeurIPS 2024). 
	This not only sheds light on why specific phenomena, such as diversity reduction and hidden bias amplification, occur in many self-improvement methods but also offers insights into mitigation
	-- namely, designing effective interaction phases to constrain unwanted biases. <br><br>

	Another line of work, which I am currently focusing on, explores the origins of the simplicity bias. 
	One theoretical tool we use is <b>learning dynamics</b>, which examines how a model’s prediction on one example changes when it learns from another. 
	This tool allows us to quantify simplicity bias through measurable properties such as learning speed or compression rate. 
	Since a model cannot learn all possible data at once, it acquires new knowledge sample by sample. 
	If learning from each example enables the model to make more accurate predictions on other samples, the training curve will decay rapidly, 
	indicating a higher compression ratio (as highlighted in this talk on <a href="https://www.youtube.com/watch?v=dO4TPJkeaaU">Compression for AGI</a>).
	We have also applied learning dynamics to explain various intriguing behaviors in deep learning, such as identifying better supervisory signals (Ren et al., ICLR 2022), 
	designing fine-tuning heads (Ren et al., ICLR 2023), and fine-tuning large language models (Ren et al., ICLR 2025).
	Interestingly, we found that self-preference amplification and simplicity bias are pervasive in gradient descent-based learning systems (Ren et al., CompLearn@NeurIPS 2024). <br><br>
	
	In addition to this microcosmic inspection of the learning process, I have recently realized that compression theory and Kolmogorov complexity-related theories offer a more macroscopic perspective on simplicity bias. 
	I believe the mechanisms underlying these concepts -- such as Occam's Razor, the <a href="https://phillipi.github.io/prh/"> Platonic Representation Hypothesis </a>, 
	learning speed advantage, and systematic generalization -- may reflect fundamental principles of learning theory. 
	I am eager to further explore this fascinating direction and uncover deeper insights.
    </p>
  </div>

<!-- The News Section -->
  <div class="w3-container w3-padding-32" id="news">
   <h2>News</h2>
      <p><li>    04/2025, our work about learning dynamics and LLM's finetuning has been selected as one of the three <font color="#DC143C">Outstanding Paper Awards at ICLR 2025!</font>
      <p><li>    04/2025, visiting <a href='https://simons.berkeley.edu/workshops/future-language-models-transformers'> Simons Institute </a> at UC Berkeley for a workshop on LLM and Transformers. 
	      Thanks for the financial support provided by <a href='https://ivado.ca/'> IVADO </a>.
      <p><li>    12/2024, attending NeurIPS-2024 at Vancouver this year. Presenting one poster about iterated learning on LLM and a workshop about simplicity bias. 
      <p><li>    12/2024, co-organizing an interesting workshop on <a href="https://language-gamification.github.io/" >Language Gamification</a> in NeurIPS 2024 @ Vancouver.
      <p><li>    09/2024, one paper accepted by NeurIPS-2024, talks about how LLM's knowledge gradually evolves if we keep conducting self-improving methods.
      <p><li>    01/2024, one paper accepted by ICLR-2024, depicting sample difficulty in NTK space.  
      <p><li>    09/2023, one paper accepted by NeurIPS-2023, iterated learning is helpful in representation learning.
      <p><li>    09/2023, start an internship at <a href="https://www.borealisai.com/research/">Borealis AI</a>, working on time series prediction project. 
      <p><li>    02/2023, one paper accepted by ICLR-2023, leave enough energy for feature adaptation.
      
      <!--
      <p><li>    12/2022, one paper presented in the 1st workshop on interpolation and beyond at NeurIPS-2022.
      <p><li>    08/2022, visiting Professor <a href="https://scholar.google.com/citations?user=km6CP8cAAAAJ&hl=en"> Aaron Courville's</a> group at 
	      		  <a href="https://mila.quebec/en/">Mila</a> for 4 months, really enjoy living in Montreal.
      <p><li>    02/2022, two papers are accepted by ICLR-2022, code and camera-ready will be released soon.
      <p><li>    10/2021, finally, after one year's waiting, I arrived in Vancouver to start my 2nd-year Ph.D. study.
      
	  10/2021, one paper has been accepted by <a href="https://icml.cc/">ICML 2021</a>.</li></p>  
      outdate news
      -->

  </div>
	
  <!-- The Talks Section -->
  <div class="w3-container w3-light-grey w3-padding-32" id="talks">
    <h2>Talks</h2>
	  	<p><li> 07/2024 Happy to share some of my understandings about the <a href="https://phillipi.github.io/prh/"> Platonic Representation Hypothesis </a>. <a href='data/MLRG_Platonic_Hypothesis.pdf'> (Slides) </a>
		<p><li> 03/2024 Happy to give a talk at Chalmers University of Technology about the application and understanding of neural iterated learning. <a href="data/neural_iterated_learning_CTH.pdf"> (Slides) </a>
	<!--
      <p><li> 06/2020, "<a href="http://valser.org/webinar/slide/slides/20200603/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-%E5%B7%A5%E4%B8%9A%E7%95%8C%E5%92%8C%E5%AD%A6%E6%9C%AF%E7%95%8C%E7%9A%84%E5%B7%AE%E5%BC%82.pdf">AI on the Edge - Discussion on the Gap Between Industry and Academia</a>" at <a  href="http://valser.org/"><strong>VALSE</strong></a> Webinar.</li></p>
      <p><li> 05/2020, "<a href="https://www.bilibili.com/video/av925692420/"> Edge AI: Progress and Future Directions</a>" at <a href="https://www.qbitai.com/"> <strong>QbitAI</strong></a> using <a  href="https://www.bilibili.com/"><strong>bilibili</strong></a>.</li></p>
	-->
  </div>

  <!-- The Notes Section -->
  <div class="w3-container w3-light-grey w3-padding-32" id="notes">
    <h2>Notes and TA</h2>
		<p><li> Here are links for TA sessions of CPSC 340 (Machine Learning and Data Mining - Fall 2024): 
		<p style='text-indent: 2em;'> <a href="data/week1.pdf"> Week 1: basic knowledge review </a> </p>
		<p style='text-indent: 2em;'> <a href="data/week2_overfit_knn.pdf"> Week 2: Variance-bias, KNN, Naive Bayes </a> </p>
		<p style='text-indent: 2em;'> <a href="data/week3_ensemble_recap_kmeans.pdf"> Week 3: Ensemble, K-means, Recap of Supervised Learning </a> </p>
		<p style='text-indent: 2em;'> <a href="data/week4_linear_regression.pdf"> Week 4: Linear Regression </a> </p>
		<p style='text-indent: 2em;'> <a href="data/week5_gradient_descent_midterm.pdf"> Week 5: Gradient Descent and Midterm </a> </p>
		<p style='text-indent: 2em;'> <a href="data/week6_new.pdf"> Week 6: Feature Selection and Regularization </a> </p>
  </div>
 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32"" id="publications">
    <h2>Publications</h2>

    <h4>Preprints:</h4>
    <ol>	    
	<p>N/A</p>
	        
    </ol>
	  
	<h4>Journal and Low-Acceptance-Rate Conference Papers:</h4>
    <ol>
	     <p>
	      <li><strong>Learning Dynamics of LLM Finetuning</strong>
	      <br>
	      <strong>Yi Ren</strong>, Danica J. Sutherland
	      <br>							    
	      <em>ICLR</em> 2025 (<font color="#DC143C">Oral, Outstanding Paper Award</font>) | 
				<a style="color: #447ec9" href="https://arxiv.org/pdf/2407.10490">pdf</a> |
		      		<a style="color: #447ec9" href="https://github.com/Joshua-Ren/Learning_dynamics_LLM">code</a>    
		                <a style="color: #447ec9" href="data/posters/ICLR2025.png">poster</a> 
	      </p>
	    
	      <p>
	      <li><strong>Bias Amplification in Language Model Evolution: An Iterated Learning Perspective</strong>
	      <br>
	      <strong>Yi Ren</strong>, Shangmin Guo, Linlu Qiu, Bailin Wang, Danica J. Sutherland
	      <br>							    
	      <em>NeurIPS</em> 2024 | 
				<a style="color: #447ec9" href="https://arxiv.org/abs/2404.04286">pdf</a> |
		      		<a style="color: #447ec9" href="https://github.com/Joshua-Ren/iICL">code</a> |
		                <a style="color: #447ec9" href="data/posters/NeurIPS2024.png">poster</a> 
	      </p>
	    
	      <p>
	      <li><strong>AdaFlood: Adaptive Flood Regularization</strong>
	      <br>
	      Wonho Bae, <strong>Yi Ren</strong>, Mohamad Osama Ahmed, Frederick Tung, Danica J Sutherland, Gabriel L Oliveira
	      <br>							    
	      <em>Transactions on Machine Learning Research (TMLR)</em> 2024 | 
				<a style="color: #447ec9" href="https://arxiv.org/abs/2311.02891">pdf</a>
	      </p>	
      <p>
      <li><strong>lpNTK: Better Generalisation with Less Data via Sample Interaction During Learning</strong>
      <br>
      Shangmin Guo, <strong>Yi Ren</strong>, Stefano V. Albrecht, Kenny Smith
      <br>							    
      <em>ICLR</em> 2024 | 
			<a style="color: #447ec9" href="https://arxiv.org/abs/2401.08808">pdf</a>
			
      </p>
	    
      <p>
      <li><strong>Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings</strong>
      <br>
      <strong>Yi Ren</strong>, Samuel Lavoie, Mikhail Galkin, Danica J. Sutherland, Aaron Courville
      <br>							    
      <em>NeurIPS</em> 2023 | 
	      		<a style="color: #447ec9" href="https://arxiv.org/pdf/2310.18777.pdf">pdf</a> |
	                <a style="color: #447ec9" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/be7430d22a4dae8516894e32f2fcc6db-Supplemental-Conference.zip">code</a> |
	                <a style="color: #447ec9" href="data/posters/NeurIPS2023.png">poster</a>
      </p>
      <p>
      <li><strong>How to prepare your task head for finetuning</strong>
      <br>
      <strong>Yi Ren</strong>, Shangmin Guo, Wonho Bae, Danica J. Sutherland
      <br>							    
      <em>ICLR</em> 2023 | 
			<a style="color: #447ec9" href="https://openreview.net/forum?id=gVOXZproe-e&referrer=[the%20profile%20of%20Yi%20Ren](/profile?id=~Yi_Ren6)">pdf</a> |
			<a style="color: #447ec9" href="https://github.com/Joshua-Ren/how_to_prepare_taskhead">code</a> |
	                <a style="color: #447ec9" href="data/posters/ICLR2023.jpg">poster</a>
      </p>
													      														
      <p>
      <li><strong>Better Supervisory Signals by Observing Learning Paths</strong>
      <br>
      <strong>Yi Ren</strong>, Shangmin Guo, Danica J. Sutherland
      <br>							    
      <em>ICLR</em> 2022 | 
			<a style="color: #447ec9" href="https://arxiv.org/pdf/2203.02485.pdf">pdf</a> |
			<a style="color: #447ec9" href="https://github.com/Joshua-Ren/better_supervisory_signal">code</a> |
	                <a style="color: #447ec9" href="data/posters/ICLR2022.png">poster</a>
      </p>
														 
      <p>
      <li><strong>Expressivity of Emergent Language is a Trade-off between Contextual Complexity and Unpredictability</strong>
      <br>
      Shangmin Guo, <strong>Yi Ren</strong>, Kory Mathewson, Simon Kirby, Stefano V. Albrecht, Kenny Smith
      <br>							    
      <em>ICLR</em> 2022 | 
			<a style="color: #447ec9" href="https://arxiv.org/pdf/2106.03982.pdf">pdf</a> |
			<a style="color: #447ec9" href="https://github.com/uoe-agents/Expressivity-of-Emergent-Languages">code</a> |											 
			<a style="color: #447ec9" href="https://arxiv.org/pdf/2012.02875.pdf">workshop-version</a>											 
      </p>										
														
      <p>
      <li><strong>Compositional languages emerge in a neural iterated learning model</strong>
      <br>
      <strong>Yi Ren</strong>, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, Simon Kirby
      <br>
      <em>ICLR</em> 2020 | 
			<a style="color: #447ec9" href="https://arxiv.org/pdf/2002.01365.pdf">pdf</a> |
			<a style="color: #447ec9" href="https://github.com/Joshua-Ren/Neural_Iterated_Learning">code</a> |
			<a style="color: #447ec9" href="https://drive.google.com/file/d/1jeSSWihY8VBfhoEgMONv71FydHro0XP0/view">workshop-version</a>
      </p>
	  	    
    </ol>

    <h4>Workshop Presentations:</h4>
	<ol>
	      <p>
	      <li><strong>Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics</strong>
	      <br>
	      <strong>Yi Ren</strong>, Danica J. Sutherland
	      <br>							    
	      <em>Compositional Learning @NeurIPS</em> 2024 | 
				<a style="color: #447ec9" href="https://arxiv.org/pdf/2409.09626">pdf</a> |
		      		<a style="color: #447ec9" href="https://github.com/Joshua-Ren/simplicity_bias_learning_dynamics">code</a> |
		                <a style="color: #447ec9" href="data/posters/WS_NeurIPS2024.png">poster</a>
	      </p>
	      <p>
	      <li><strong>Economics arena for large language models</strong>
	      <br>
	      Shangmin Guo, Haoran Bu, Haochuan Wang, <strong>Yi Ren</strong>, Dianbo Sui, Yuming Shang, Siting Lu
	      <br>							    
	      <em>Language Gamification @NeurIPS</em> 2024 | 
				<a style="color: #447ec9" href="https://arxiv.org/abs/2401.01735">pdf</a>
	      </p>
		
	      <p>
	      <li><strong>The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents</strong>
	      <br>
	      Shangmin Guo, <strong>Yi Ren</strong>, Serhii Havrylov, Stella Frank, Ivan Titov, Kenny Smith
	      <br>
	      <em>EmeCom @NeurIPS</em> 2019 | 
				<a style="color: #447ec9" href="https://arxiv.org/pdf/1910.05291.pdf">pdf</a>
	      </p>		
  	</ol>
<!--	<h4>Wireless Communications:</h4>
	<ol>
      <p>
      <li><strong>Location-partition-based resource allocation in D2D-supported vehicular communication networks</strong>
      <br>
      Meiyan Wu, <strong>Yi Ren</strong>, Ping Wang, Chao Wang, Yusheng Ji
      <br>
      <em>IEEE VTC-Spring</em> 2018 | 
			<a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/8417668">pdf</a>
      </p>	
	  
      <p>
      <li><strong>Power control in D2D-based vehicular communication networks</strong>
      <br>
      <strong>Yi Ren</strong>, Fuqiang Liu, Zhi Liu, Chao Wang, Yusheng Ji
      <br>
      <em>IEEE Trans. Veh. Technol.</em> 2015 | 
			<a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/7289470">pdf</a>
      </p>		

      <p>
      <li><strong>Applying LTE-D2D to support V2V communication using local geographic knowledge</strong>
      <br>
      <strong>Yi Ren</strong>, Chao Wang, Dong Liu, Fuqiang Liu, Erwu Liu
      <br>
      <em>IEEE VTC-Fall</em> 2015 | 
			<a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/7390845">pdf</a>
      </p>		

      <p>
      <li><strong>Bounds on secondary user connectivity in cognitive radio networks</strong>
      <br>
      Dong Liu, Erwu Liu, <strong>Yi Ren</strong>, Zhengqing Zhang, Rui Wang, Fuqiang Liu
      <br>
      <em>IEEE Commun. Lett.</em> 2015 | 
			<a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/7042738">pdf</a>
      </p>		  
	</ol>
-->
	
    </p>
  </div>

  <div class="w3-light-grey w3-center w3-padding-24">

    Welcome to my home page *^_^* </a></br>

  <!-- Default Statcounter code for Yunhe Wang's Homepage
  https://joshua-ren.github.io -->
  No. 
  <script type="text/javascript">
	var sc_project=12700421; 
	var sc_invisible=0; 
	var sc_security="7402a5b2"; 
	var scJsHost = "https://";
	document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
	"statcounter.com/counter/counter.js'></"+"script>");
  </script> Visitor Since Jan 2022. Powered by <a href="https://www.w3schools.com/w3css/default.asp" title="W3.CSS" target="_blank" class="w3-hover-opacity">w3.css</a>
  <noscript>
    <div class="statcounter"><a title="Web Analytics Made Easy -
  StatCounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12347113/0/21aca5d1/0/"
  alt="Web Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>
